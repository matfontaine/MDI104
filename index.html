<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>MDI104 - Télécom Paris</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/telecom.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/github.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section class="cover" data-background="figures/background-blur.png"  data-state="no-title-footer no-progressbar has-dark-background">

					<h2 id='coverh2'>Probabilités</h2>
					<h1  id='title_seminar'> MDI104 </h1>
					<h3><a href="https://matfontaine.github.io/MDI104", id='github_url'>matfontaine.github.io/MDI104</a></h3>
					<p id='coverauthors'>
						Mathieu FONTAINE<br />
						mathieu.fontaine@telecom-paris.fr
					</p>
					<p id="date">
					Septembre-Decembre 2022
					</p>
					<p>
					<img src="css/theme/img/logo-Telecom.svg" id="telecom" class="logo" alt="">
					<aside class="notes">
						<ul><li>We will consider historical audio source separation technique</li>
									<li>e.g. no deep learning extensions or nonnegative matrix factorization</li>
								<li>the Handbook for that course is available on the moodle (PAM/Audio_source_separation)</li>
						</ul>
					</aside>
				</section>

				<!-- Outline of the presentation -->
				<section>
					<h1> Organisation du module (1/2)</h1>
					<h2>Evaluation</h2>

					<h3> Note de contrôle continu  (CC, /10)</h3>
					<ul>
						<li>2 CC (/20)

							<ul>
								<li>Date pour les contrôles continus : <b>30/09</b> et <b>18/11</b></li>
								<li>1h30 - une feuille A4 recto/verso de révision autorisée pendant le devoir</li>
							</ul>

						</li>
						<li> Devoir Maison (/10) : à faire tout seul ou en binôme (à rendre pour le lundi <b>26/09</b>)</li>
					</ul></br></br>

     <p class="remarque">Note sur /50 ramenée sur 10</p>               
					<h3>Examen Final (EF, /10)</h3>
					<ul style="margin-bottom:1em;">
						<li>Une feuille A4 recto/verso de révision autorisée pendant le devoir</li>
						<li>Durée : 3h</li>
					</ul>
					<p class="remarque"> EF + CC = Note finale /20 pour MDI104</p>
				</section>

				<section>
					<h1> Organisation du module (2/2)</h1>
					<h2>Programme par tranche horaire (TH)</h2>
					<ul>
						<li>Probabilités Discrète (TH 1-2)</li>
						<li>Théorie de la Mesure (TH 3-5)</li>
						<li>Intégration (TH 8-9)</li>
						<li>Variables Aléatoires et Espérance (TH 10-12)</li>
						<li><font color ="red">CC1 (TH 13, 30/09)</font></li>
						<li>Théorème de Fubini et Indépendance (TH 14)</li>
						<li>Changement de Variables (TH 15-16)</li>
						<li>Fonction Caractéristique (TH 17-18)</li>
						
						<li>Vecteurs Gaussiens (TH 19-21)</br>
							<b>(TH 19-20 le 28/10 remplacement par Bruno Costacèque)</b>

						</li>
						<li>Espérance conditionnelle (TH 22 & 24)</li>
						<li><font color ="red">CC2 (TH 23, 18/11)</font></li>
						<li>Convergences (TH 25 & 26)</li>
				    </ul>
					</section>

					<section>
						<h1>Matériel et activités</h1>
						<h2>Bibliographie</h2>
						<ul>
							<li>Le polycopié du MDI104: contient des exercices corrigés</li>
							<li>Cours de Jean-François Delmas (Partie I) <a href="http://cermics.enpc.fr/%7Edelmas/Enseig/ensta_cours.pdf"> Téléchargeable ici</a></li>
							<li>Probability and Measure - Patrick Billingsley <a href="https://www.colorado.edu/amath/sites/default/files/attached-files/billingsley.pdf"> Téléchargeable ici</a></li>
      					</ul>
						<h2>Activités</h2>
						<ul>
							<li>Diaporama résumant le contenu du cours (démonstration au tableau)</li>
							<li>Travaux dirigés sur des exercices (à la maison ou traité directement en cours)</li>
							<!-- <li>Petit QCM pendant le cours (non noté)
							   </br>$\quad \rightarrow$ Matériel nécessaire : <b>Téléphone ou ordinateur</b> (test juste après)
							</li> -->
						</ul>
						</section>

					<!-- <section>
						<h1> QCM Wooclap</h1>
						<iframe style="pointer-events: none;" frameborder="0" height="500" width="100%" mozallowfullscreen src="https://app.wooclap.com/events/MDI104GRP2/"></iframe>
						<ul>
							<li>www.wooclap.com/MDI104GRP2 ou  @MDI104GRP2 par SMS et 1,2,3 ou 4 etc.</li>
							<li>Les questions sont limitées par le temps <b>(n'est pas pris en compte dans la note)</b></li>
						</ul>
					</section> -->

				<!-- Introduction -->
				<section class="cover" data-background="figures/background.png" data-state="no-title-footer no-progressbar has-dark-background">
					<h2 id='coverh2'>I - Probabilités discrète</h2>

				</section>

				<section>
					<h1>Rappels & notations (1/2)</h1>
					<ul>
						<li> $\Omega$: ensemble de réalisation possible (parfois appelé <b>Univers</b>)</li>
						$\quad \rightarrow$ durée de vie d'une population (continue, $\Omega = \mathbb{R}_{+}$)$ \\ $ 
					    $\quad \rightarrow$ comptage d'objet défaillant durant une période donnée ($\Omega = \mathbb{N}$)$\\$ 
						<p class="remarque"> Dans cette 1$^{\mathrm{ère}}$partie, $\Omega$ sera au plus égal à $\mathbb{N}$</p>
						<li>$\omega \in \Omega$: <b>épreuve, issue</b> </li>
						$\quad \rightarrow$ représente le résultat d'un(e) phénomène/expérience aléatoire. 
						<li>$A \subset \Omega$ ou $A \in \mathcal{P}(\Omega)$ (partie de $\Omega$): évènement aléatoire</li>
						<!-- $\quad \rightarrow$ $A$ est <b>réalisé</b> ssi. le résultat de l'expérience $\omega \in A$ -->
					</ul>
					<div class="exemple"> 
					<div id="title"> Exemple (lancé de dés) : </div> 
					Considérons 2 dés et l'évènement $A=\{$ Faire au moins 10 après un lancer de 2 dés$\}$. 
					On a l'ensemble $\Omega=\left\{1,\dots,6\right\} \times \left\{1,\dots,6\right\}$ et 
					<center>$$
					A = \left\{(\omega_1,\omega_2) \in \Omega \mid \omega_1 + \omega_2 \geq 10 \right\}
					$$</center>
		
					</div>
					<center><img src="figures/images/2_des.jpg" width="10%" style="margin-top:1em"></center>
					
				</section>

				<section>
					<h1>Rappels & notations (2/2)</h1>
					Dans ce contexte, on rappelle également que:$\\$
					<ul>
						<li>$\bigcup_{i \in \mathbb{N}} A_i = \{\omega \in \Omega \mid  \exists i \in \mathbb{N}, \omega \in A_i \}$</li>
						<li> $\bigcap_{i \in \mathbb{N}} A_i = \{\omega \in \Omega \mid  \forall i \in \mathbb{N}, \omega \in A_i \}$</li>
						<li> Soit $I \subset \mathbb{N}$, <b>$(A_i)_{i \in I}$  partition de $\Omega$ </b>$  \Leftrightarrow \forall i \neq j, A_i \cap A_j = \emptyset$ et $\Omega = \bigcup_{i \in \mathbb{N}} A_i$</li>
						<li> $A, \bar{A}$ forment une partition de $\Omega$</li>
					</ul>
					$\\$
					Notion de limite de suite croissante/décroissante d'évènements :
					<ul>
						<li>Si $(A_i)_i$ tel que $A_i \subset A_{i+1}$ alors on note $A:= \lim_{i} A_i = \bigcup_{n} A_i$ ou $A_i \uparrow A$ </li>
						<li>Si $(A_i)_i$ tel que $A_{i+1} \subset A_{i}$ alors on note $A:= \lim_{i} A_i = \bigcap_{n} A_i$ ou $A_i \downarrow A$</li>
					</ul> 
					<center><img src="figures/images/suite_croi_dec.png" width="100%" style="margin-top:1em"></center>
				</section>

				<section>
					<h1>Mesure de probabilité</h1>
					Intuitivement, on souhaite définir une application/mesure telle que :
					<ul style="margin-bottom:1.3em;">
					<li>la mesure de l'union d'ensembles disjoints soit la somme de la mesure de chaque ensemble </li>
					<li>mesure de  l'ensemble vide $=0$ et mesure de  $\Omega = 1$ (probabilité)</li>
				</ul>
					<div class="exemple" style="margin-bottom:1em;"> 
						<div id="title"> Définition (Mesure de probabilité) : </div> 
						Une <b>mesure de probabilité</b> sur $\Omega$ est une application 
						<center>$\mathbb{P}: \mathcal{P}(\Omega) \to \mathbb{R} \\ $
							$ \qquad \qquad ~ A \mapsto \mathbb{P}(A)$
						</center>
						Qui vérifie </br>
						$\qquad$
						<ol>
							<li> $\mathbb{P}(\emptyset) = 0$ et $\mathbb{P}(\Omega)=1$</li>
							<li> $\forall (A_i)_{i \in \mathbb{N}}$ d'évènements 2 à 2 disjoints <br>
								<center>$\mathbb{P}(\bigcup_{i \in \mathbb{N}} A_i) = \sum_{i \in \mathbb{N}} \mathbb{P}(A_i) \qquad \qquad \qquad  \qquad \qquad  (\sigma\texttt{-additivité})$</center>


							</li>
						</ol>
			
						</div>
				</section>
				<section>
				<h1>Propriétés</h1>
				<div class="exemple"> 
					<div id="title"> Propriétés sur la mesure de probabilité : </div> 
					Soit $\mathbb{P}$ une mesure de probabilité. Elle satisfait les propriétés suivantes : 
					<ol style="margin-left:1.8em;">
						<li> $\mathbb{P}(\bar{A}) = 1-\mathbb{P}(A)$</li>
						<li>$\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)$</li>
						<li>Si $A \subset B$ alors $\mathbb{P}(A) \leq \mathbb{P}(B)$</li>
						<li>Si $(A_i)_i$ forme une partition de $\Omega$ alors pour tout $B \subset \Omega$
							<center>$\mathbb{P}(B) = \sum_{i} \mathbb{P}(B \cap A_i) \quad  (\texttt{Formule des probabilités totales})$</center> 
						</li>
						<li>
							$\mathbb{P}(\cup_{i}A_i) \leq \sum_{i} \mathbb{P}(A_i) \quad ~~~ (\texttt{Borne de l'union})$
						</li>
						<li>
							Si </br>
							$\qquad \rightarrow A_n \uparrow A$, alors $\mathbb{P}(A) = \lim_{n \to +\infty} \mathbb{P}(A_n) \\$
							$\qquad \rightarrow A_n \downarrow A$, alors $\mathbb{P}(A) = \lim_{n \to +\infty} \mathbb{P}(A_n) \\$
						</li>
						<li>
							Si $\forall n \in \mathbb{N}^{\star}, \mathbb{P}(A_n) =1$ alors $\mathbb{P}(\bigcap_{n=1}^{\infty} A_n) = 1$
						</li>
					</ol>
					
					</div>
					<!-- <p><center><b>Esquisse de preuve au tableau. </b></center>
					</p> -->
				</section>

				<section>
					<h1>Probabilité conditionnelle</h1>
					 Proba conditionnelle de $A$ sachant $B \rightarrow$ quantifie l'occurence $A$ sachant que $B$ s'est produit. 
					 <div class="exemple"> 
						<div id="title"> Définition (Probabilité conditionnelle) : </div> 
						Soit $\mathbb{P}$ une mesure de probabilité et $B$ tel que $\mathbb{P}(B)>0$.
						 On définit la <b>probabilité conditionnelle de $A$ sachant $B$</b> comme suit : $\\$
						<center>$\mathbb{P}(A\mid B) =\dfrac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}$</center>

						</div>
						<p>
						 <b>Remarque </b>: $A \mapsto \mathbb{P}(A \mid B)$ est une mesure de probabilité <b> (vérifier les axiomes)</b>
		
						</p>

						<div class="exemple"> 
							<div id="title"> Propriétés : </div> 
							<ol style="margin-left:1.8em;">
							<li>Si on considère $(B_i)_{i \in I}$ une partition de $\Omega$ alors: $\\$
							<center>$\mathbb{P}(A) = \sum_{i \in I} \mathbb{P}(A \mid B_i) \mathbb{P}(B_i)$</center>
						</li>
						<li>Pour tout évènement $A$ et $B$ on a:
							<center>$ \qquad \qquad \qquad\mathbb{P}(B \mid A) = \dfrac{\mathbb{P}(A \mid B) \mathbb{P}(B)}{\mathbb{P}(A)}$ 
								$ \quad  \texttt{(Formule de Bayes)} $</center>
								

						</li>
				     	</ol>	
						</div>
							<!-- <p><center><b>Preuve au tableau. </b></center>
							</p> -->
				</section>
				<section>
					<h1>Indépendance</h1>

					<div class="exemple"> 
						<div id="title"> Definition : </div> 
						<ol style="margin-left:1.8em;">
						<li>$A$ et $B$ sont dits <b>indépendants</b> ($A \perp \! \! \! \perp B$) si <center>$\mathbb{P}(A\cap B) = \mathbb{P}(A) \mathbb{P}(B) \Leftrightarrow \mathbb{P}(A \mid B) = \mathbb{P}(A)$</center>

						</li>
					<li> Une famille d'évènements $(A_i)_{i \in I}$ est dite <b>indépendante</b> si <br>
						 pour tout ensemble fini $J \subset I$ la sous-famille $(A_j)_{j \in J}$  vérifie :

						<center>$\mathbb{P}(\bigcap_{j \in J}A_j) = \prod_{j \in J} \mathbb{P}(A_j)$</center>
							

					</li>
					 </ol>	
						</div>

                     <p><b>Exemple :</b> $A \perp \! \! \! \perp B \perp \! \! \! \perp C
						 \Leftrightarrow 
						 \begin{cases}
						  \mathbb{P}(A \cap B) &= \mathbb{P}(A)\mathbb{P}(B); \\
						 \mathbb{P}(A \cap C) &= \mathbb{P}(A)\mathbb{P}(C); \\
						 \mathbb{P}(A \cap B \cap C) &= \mathbb{P}(A)\mathbb{P}(B)\mathbb{P}(C).
						 \end{cases}
						 $</p>
					<!-- <p class="remarque"> Petit QCM juste après sur Wooclap (Proba conditionnelle & indépendance)</p>  -->
				</section>
				<!-- <section>
				<h1>QCM Wooclap (Proba conditionnelle & indépendance) </h1>
				<iframe style="pointer-events: none;" frameborder="0" height="500" width="100%" mozallowfullscreen src="https://app.wooclap.com/events/MDI104GRP2/"></iframe>
				<ul>
					<li>www.wooclap.com/MDI104GRP2 ou  @MDI104GRP2 par SMS et 1,2,3 ou 4 etc.</li>
					<li>Les questions sont limitées par le temps <b>(n'est pas pris en compte dans la note)</b></li>
				</ul>
				</section> -->
				<section>
					<h1>Variable aléatoire</h1>
				$\Omega$ et $E$ sont des espaces discrets.

				<div class="exemple" style="margin-bottom:1em;"> 
					<div id="title"> Définition (Variable aléatoire) </div> 
					Une variable aléatoire (v.a.) $X$ sur $E$ est une fonction  $X:\Omega \to E$.
					</div>
				$X(\omega)$ est parfois appelé une réalisation de $X \\$
				$\quad \rightarrow$ elle dépend du résultat d'une expérience. $\\ \\$

				On s'intéresse aux évènements suivants associés à une v.a. $X~: \\$
				<ul>
				<li>$A=X^{-1}(\{x\}) \rightarrow $"La variable $X$ prend la valeur $x$"</li>
				<li>$A=\{\omega \in \Omega \mid X(\omega) \in H\} := X^{-1}(H) \rightarrow $"La variable $X$ appartient à $H$"</li>
				<li>$X^{-1}(H)$ est appelé <b>l'image réciproque de $H$ par $X$</b></li>
				

				</ul>
				</section>

				<section>
					<h1>Loi d'une variable aléatoire</h1>

				<div class="exemple" style="margin-bottom:1em;"> 
					<div id="title"> Définition (Loi d'une variable aléatoire) </div> 
					La <b>loi</b> d'une v.a. $X$ est l'application
					<center>$\mathbb{P}_X: E \to \mathbb{R} \\ $
						$ \qquad \qquad \quad \qquad \qquad ~ H \mapsto \mathbb{P}(X^{-1}(H))$
					</center>
					</div>
					
					<p>
					<b>Notations:</b> On note  $\mathbb{P}_X = \mathbb{P} \circ X^{-1}$ mais également :
					<center>
                    $$\begin{aligned}
					\mathbb{P}(X^{-1}(H))  &=\mathbb{P}(\{\omega \in \Omega: X(\omega) \in H\})\\
										   &:=\mathbb{P}(\{X \in H\}) \\
										   &:=\mathbb{P}(X \in H) \\
					\end{aligned}
					$$
	

					</center>
					On a de plus les notations équivalentes suivantes :
					<center>$\mathbb{P}_{X}(H) := \mathbb{P}(X \in H) = \sum_{x \in H} \mathbb{P}_{X}(x) = \sum_{x \in H} \mathbb{P}(X=x)$</center> 

					</p>
					<div class="exemple" style="margin-bottom:1em;"> 
						<div id="title"> Propriété </div> 
						$\mathbb{P}_{X}$ est une mesure de probabilité
						</div>

						<p><center><b>Preuve dans le poly. </b></center>
						</p>
				</section>
				<section>
					<h1>Loi jointe, loi marginale</h1>
					<div class="exemple" style="margin-bottom:1em;"> 
						<div id="title"> Définition (Loi jointe, loi marginale) </div> 
						Soit $X$ et $Y$ deux v.a; de $\Omega$ dans $E$ de lois respectives $\mathbb{P}_{X}$ et $\mathbb{P}_{Y}$. La loi du couple $(X,Y)$, 
						notée $\mathbb{P}_{(X,Y)}$, s'appelle <b>la loi jointe de $X$ et $Y$</b>.  </br>
						Les lois de $\mathbb{P}_{X}$ et $\mathbb{P}_{Y}$ sont respectivement <b>les marginales de $X$ et $Y$</b>.
						</div>
					<p>
						La marginale peut être retrouvée à partir de la loi jointe via la relation suivante :
					</p>
						<div class="exemple" style="margin-bottom:1em;"> 
						<div id="title"> Propriété </div> 
						<center>
							$\forall x \in E, \mathbb{P}_{X}(x) = \sum_{y \in E} \mathbb{P}_{(X,Y)}(x,y)$</b>.
						</center>
						</div>
					<b>Preuve : Appliquer la formule des probabilités totales sur les ensembles $\{Y=y\}$ où $y \in E$.</b>
				</section>
				<section>
					<h1>Indépendance de variables aléatoires</h1>
					<div class="exemple" style="margin-bottom:1em;"> 
						<div id="title"> Définition (Indépendance de v.a.) </div> 
						Deux v.a. $X$ et $Y$ sont dites <b>indépendantes </b> si pour tout $A,B \subset E$ on a l'indépendance des évènements
						$\{X \in A\}$ et $\{Y \in B\}$. C'est-à-dire :
						<center>
							$\forall A,B \subset E, \underbrace{\mathbb{P}(\{X \in A\} \cap \{Y \in B\})}_{=\mathbb{P}(X \in A,~ Y \in B)}=\mathbb{P}(X \in A)\mathbb{P}(X \in B)$

						</center>
						</div>
						<b>Remarque:</b> On retrouve parfois la notation $\mathbb{P}(X\in A, Y\in B) = \mathbb{P}_{(X,Y)}(A \times B)$.
						$\\$ En effet $$
						\begin{aligned}
						\mathbb{P}(X\in A, Y\in B) &= \mathbb{P}(\{\omega \in \Omega \mid X(\omega)\in A, Y(\omega)\in B\})\\
						 						   &= \mathbb{P}(\{\omega \in \Omega \mid (X(\omega),Y(\omega))\in A\times B\})\\
												   &= \mathbb{P}_{(X,Y)}(A \times B)
						\end{aligned}						   $$
						<div class="exemple" style="margin-bottom:1em;"> 
							<div id="title"> Propriété </div> 
							$$ \begin{aligned}
							    X \perp \! \! \! \perp Y &\Leftrightarrow \forall (A,B) \subset \Omega^2, \mathbb{P}_{(X,Y)}(A \times B) = \mathbb{P}_{X}(A) \mathbb{P}_{Y}(B)\\
								&\Leftrightarrow \forall (x,y) \in \Omega^2, \mathbb{P}(X=x,Y=y) =  \mathbb{P}(X=x) \mathbb{P}(Y=y)
								
								\end{aligned}
								$$
							</div>
							<b>Preuve: Au tableau.</b>
					</section>
				<section>
					<h1>Espérance</h1>
					<div class="exemple" style="margin-bottom:1em;"> 
						<div id="title"> Définition (Espérance)</div> 
						Soit $E \subset \mathbb{R}$, <b>l'espérance d'une v.a. $X$ </b> est définie (quand elle existe) par :
						<center>
						$$\mathbb{E}(X) = \sum_{x \in E} x \mathbb{P}(X=x)$$

						</center>
						</div>
						<p>
						<b>Remarque :</b> L'espérance existe si :
						<ul>
							<li> la série $\mathbb{E}(X)$ est absolument sommable $\left(\sum_x |x|\mathbb{P}(X=x) < \infty \right)$</li>
							<li>$\forall x < 0, \mathbb{P}(X=x) = 0 \rightarrow$ que $X$ est positive $\mathbb{P}$-presque partout $(X \geq 0)$</li>
						</ul>
					</p>
				<p>
					<b>Exemple :</b> On considère la fonction indicatrice : 
					<center>$$ 
					
					\begin{aligned}
					\bold{1}_{A}: \Omega &\to  \{0,1\}\\
					  \omega & \mapsto  
					  \begin{cases}
					  1 & \mathrm{si~\omega \in A} \\
					  0 & \mathrm{sinon}
					\end{cases}
					  \end{aligned} 
					  $$
					</center>
				</p>
				Alors: $\mathbb{E}(\bold{1}_{A}) = 0\mathbb{P}(\bold{1}_{A}=0) + 1\mathbb{P}(\bold{1}_{A}=1) = \mathbb{P}(A) \implies \boxed{\mathbb{E}(\bold{1}_{A}) = \mathbb{P}(A)}
				  $
				</section>

				<section>
					<h1>Propriétés autour de l'éspérance (1/2)</h1>

					On définit l'égalité $\mathbb{P}$- presque partout ($\mathbb{P}$-p.p.) c'est à dire avec probabilité 1 ($X=a ~\mathbb{P}$-p.p. veut dire $\mathbb{P}(X=a) = 1$)
					<div class="exemple" style="margin-bottom:1em;"> 
						<div id="title"> Propriétés</div> 
						Soit $X$ et $Y$ deux v.a. dans $E$ telles que $\mathbb{E}(|X|) < +\infty$ et $\mathbb{E}(|Y|) < +\infty, \alpha, \beta \in \mathbb{R}$ et $a \in E$. Alors :
						<ol style="margin-left:1.8em;">  
							<li style="margin-top:0.5em;">$\hspace{-2em} \mathbb{E}(\alpha X + \beta Y)$ est bien définie et $\mathbb{E}(\alpha X + \beta Y) = \alpha\mathbb{E}(X) + \beta\mathbb{E}(Y)$ </li>
							<li style="margin-top:0.5em;"> $\hspace{-2em} \text{Si } X \geq 0~\mathbb{P}-$ p.p. alors $\mathbb{E}(X) \geq 0$</li>
							<li style="margin-top:0.5em;"> $\hspace{-2em}\text{Si } X \geq 0~\mathbb{P}-$ p.p. et $\mathbb{E}(X) = 0$ alors $X=0~\mathbb{P}-$ p.p.</li>
							<li style="margin-top:0.5em;">$\hspace{-2em} \left|\mathbb{E}(X)\right| \leq \mathbb{E}(\left|X\right|)$  </li>
							<li style="margin-top:0.5em;">$\hspace{-2em}\text{Si } X \leq Y~\mathbb{P}-$ p.p. alors $\mathbb{E}(X) \leq \mathbb{E}(Y)$</li>
							<li style="margin-top:0.5em;">$\hspace{-2em}\text{Si } X = a~\mathbb{P}-$ p.p. alors $\mathbb{E}(X) = a$</li>
						</ol>
						</div>
					</section>

				<section>
				<h1>Propriétés autour de l'éspérance (2/2)</h1>
				<div class="exemple" style="margin-bottom:1em;"> 
					<div id="title"> Propriétés</div> 
					Soit $X$ et $Y$ deux v.a. dans $E$ telles que $\mathbb{E}(|X|) < +\infty$ et $\mathbb{E}(|Y|) < +\infty$ et $ \\g: E \to \mathbb{R}$. Alors :
					<ol style="margin-left:1.8em;">  
						<li style="margin-top:0.5em;">$\hspace{-2em} \text{Les résultats élémentaires de Prop. 1.16.}$ </li>
						<li style="margin-top:0.5em;"> $\hspace{-2em}\forall \epsilon >0, \forall p \geq 1, \mathbb{P}(|X|>\epsilon) \leq \dfrac{\mathbb{E}(|X|^p)}{\epsilon^p} ~~ (\texttt{Inégalité de Markov})$</li>
						<li style="margin-top:0.5em;">$\hspace{-2em}\mathbb{E}(|XY|) \leq \sqrt{\mathbb{E}(X^2)\mathbb{E}(Y^2)}~~~~ (\texttt{Inégalité de Cauchy-Schwarz})$</li>
						<li style="margin-top:0.5em;">$\hspace{-2em} \mathbb{E}(g(X)) = \sum_{x\in E}g(x)\mathbb{P}(X=x) \quad ~~~ (\texttt{Théorème de transfert})$  </li>
						<li style="margin-top:0.5em;">$\hspace{-2em} \text{Si~} X \perp \! \! \! \perp Y$ alors $\mathbb{E}(f(X)g(Y)) = \mathbb{E}(f(X))\mathbb{E}(g(Y))$</li>
					</ol>
					</div>
					<p><b>Preuve : au tableau pour ii. et v.</b></p>
					<p><b>Remarque :</b> iv. peut se généraliser pour $X_1,\dots,X_n$ et $g:E^n \to \mathbb{R}$:
					<center>
					$$
					\mathbb{E}(g(X_1,\dots,X_n)) = \sum_{(x_1,\dots,x_n)\in E^n}g(x_1,\dots,x_n)\mathbb{P}(X_1=x_1,\dots,X_n=x_n)
					$$

					</center>
					</p>
				</section>
				<section>
					<h1>Moments, variances et covariance</h1>
					<div class="exemple" style="margin-bottom:1em;"> 
						<div id="title"> Définition</div> 
						Soit $p \geq 0$. et soit $X$ une v.a. tel que $\mathbb{E}(|X|^p)<+\infty$. Alors :
						<ol style="margin-left:1.8em;">  
							<li style="margin-top:0.5em;">$\hspace{-3em}\mathbb{E}(X^p)$ est appellé le <b>moment d'ordre $p$ de $X$</b>. $X$ est alors dit d'ordre $p$.   </li>
							<li style="margin-top:0.5em;"> $X$ d'ordre 2. Sa <b>variance</b>, notée $\mathrm{Var}(X)$,  est définie par:
							<center>$$\mathrm{Var}(X):= \mathbb{E}\left[(X-\mathbb{E}(X))^2\right] $$</center>

							</li>
							<li style="margin-top:0.5em;">$X,Y $deux v.a. d'ordre $2$. On définit leur <b>covariance</b> $\mathrm{Cov}(X,Y)$ par:
								<center>$$\mathrm{Cov}(X,Y):= \mathbb{E}\left[(X-\mathbb{E}(X)(Y-\mathbb{E}(Y))\right] $$</center>
								Si $\mathrm{Cov}(X,Y)=0$ $X,Y$ sont dits <b>décorrélées</b>. 
							</li>
						</ol>
					</div>

					Rappelons quelques propriétés importantes parmis celles du polycopié :
				
					<div class="exemple" style="margin-bottom:1em;"> 
						<div id="title"> Propriétés</div> 
						Soit $X,Y$ deux v.a. d'ordre $2$. Alors:$\\$
						<ol style="margin-left:1.8em;">  
							<li style="margin-top:0.5em;">$ \hspace{-3em} \text{Les résultats élémentaires de Prop. 1.20. sur la Variance/Covariance}$</li>
							<li style="margin-top:0.5em;">$\hspace{-3em} $ $X \perp \! \! \! \perp Y \implies \mathrm{Cov}(X,Y) =0$ (⚠ le contraire est généralement faux)</li>
							<li style="margin-top:0.5em;">$\hspace{-3em} $ $X$ et $Y$ ont mêmes lois $\implies$ leurs moments sont égaux.</li>

						</ol>

					</div>
				</section>
</div>

<div class='footer'>
	<img src="css/theme/img/logo-Telecom.svg" alt="Logo"/>
	<div id="middlebox">Probabilités - MDI104</div>
	<ul>
	</ul>
</div>
			</div>

		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				controls: false,
				progress: true,
				history: true,
				center: false,
				slideNumber: true,
				minScale: 0.1,
				maxScale: 5,
				transition: 'none', //

				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math-katex/math-katex.js', async: true },
					{ src: 'plugin/reveald3/reveald3.js' },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>

	</body>

</html>
